{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Kernel can **`automatically orchestrate` different plugins by using a `planner`**. With the planner, a user can ask your application to achieve a complex goal. For example, if you have a function that identifies which animal is in a picture and another function that tells knock-knock jokes, your user can say, “Tell me a knock-knock joke about the animal in the picture in this URL,” and the planner will automatically understand that it needs to call the identification function first and the “tell joke” function after it. **Semantic Kernel will automatically search and combine your plugins to achieve that goal and create a plan.** Then, Semantic Kernel will execute that plan and provide a response to the user:\n",
    "\n",
    "<img src=\"../assets/semantic-kernel-architecture.png\" />\n",
    "\n",
    "\n",
    "**`LLM Cascade:`** The process of sending simpler requests to simpler models and complex requests to more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Kernel in module semantic_kernel.kernel object:\n",
      "\n",
      "class Kernel(semantic_kernel.filters.kernel_filters_extension.KernelFilterExtension, semantic_kernel.functions.kernel_function_extension.KernelFunctionExtension, semantic_kernel.services.kernel_services_extension.KernelServicesExtension, semantic_kernel.reliability.kernel_reliability_extension.KernelReliabilityExtension)\n",
      " |  Kernel(plugins: semantic_kernel.functions.kernel_plugin.KernelPlugin | dict[str, semantic_kernel.functions.kernel_plugin.KernelPlugin] | list[semantic_kernel.functions.kernel_plugin.KernelPlugin] | None = None, services: Union[~AI_SERVICE_CLIENT_TYPE, list[~AI_SERVICE_CLIENT_TYPE], dict[str, ~AI_SERVICE_CLIENT_TYPE], NoneType] = None, ai_service_selector: semantic_kernel.services.ai_service_selector.AIServiceSelector | None = None, *, retry_mechanism: semantic_kernel.reliability.retry_mechanism_base.RetryMechanismBase = None, function_invocation_filters: list[tuple[int, collections.abc.Callable[[~FILTER_CONTEXT_TYPE, collections.abc.Callable[[~FILTER_CONTEXT_TYPE], None]], None]]] = None, prompt_rendering_filters: list[tuple[int, collections.abc.Callable[[~FILTER_CONTEXT_TYPE, collections.abc.Callable[[~FILTER_CONTEXT_TYPE], None]], None]]] = None, auto_function_invocation_filters: list[tuple[int, collections.abc.Callable[[~FILTER_CONTEXT_TYPE, collections.abc.Callable[[~FILTER_CONTEXT_TYPE], None]], None]]] = None) -> None\n",
      " |\n",
      " |  The Kernel of Semantic Kernel.\n",
      " |\n",
      " |  This is the main entry point for Semantic Kernel. It provides the ability to run\n",
      " |  functions and manage filters, plugins, and AI services.\n",
      " |\n",
      " |  Attributes:\n",
      " |      function_invocation_filters: Filters applied during function invocation, from KernelFilterExtension.\n",
      " |      prompt_rendering_filters: Filters applied during prompt rendering, from KernelFilterExtension.\n",
      " |      auto_function_invocation_filters: Filters applied during auto function invocation, from KernelFilterExtension.\n",
      " |      plugins: A dict with the plugins registered with the Kernel, from KernelFunctionExtension.\n",
      " |      services: A dict with the services registered with the Kernel, from KernelServicesExtension.\n",
      " |      ai_service_selector: The AI service selector to be used by the kernel, from KernelServicesExtension.\n",
      " |      retry_mechanism: The retry mechanism to be used by the kernel, from KernelReliabilityExtension.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Kernel\n",
      " |      semantic_kernel.filters.kernel_filters_extension.KernelFilterExtension\n",
      " |      semantic_kernel.functions.kernel_function_extension.KernelFunctionExtension\n",
      " |      semantic_kernel.services.kernel_services_extension.KernelServicesExtension\n",
      " |      semantic_kernel.reliability.kernel_reliability_extension.KernelReliabilityExtension\n",
      " |      semantic_kernel.kernel_pydantic.KernelBaseModel\n",
      " |      pydantic.main.BaseModel\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, plugins: semantic_kernel.functions.kernel_plugin.KernelPlugin | dict[str, semantic_kernel.functions.kernel_plugin.KernelPlugin] | list[semantic_kernel.functions.kernel_plugin.KernelPlugin] | None = None, services: Union[~AI_SERVICE_CLIENT_TYPE, list[~AI_SERVICE_CLIENT_TYPE], dict[str, ~AI_SERVICE_CLIENT_TYPE], NoneType] = None, ai_service_selector: semantic_kernel.services.ai_service_selector.AIServiceSelector | None = None, **kwargs: Any) -> None\n",
      " |      Initialize a new instance of the Kernel class.\n",
      " |\n",
      " |      Args:\n",
      " |          plugins: The plugins to be used by the kernel, will be rewritten to a dict with plugin name as key\n",
      " |          services: The services to be used by the kernel, will be rewritten to a dict with service_id as key\n",
      " |          ai_service_selector: The AI service selector to be used by the kernel,\n",
      " |              default is based on order of execution settings.\n",
      " |          **kwargs: Additional fields to be passed to the Kernel model,\n",
      " |              these are limited to retry_mechanism and function_invoking_handlers\n",
      " |              and function_invoked_handlers, the best way to add function_invoking_handlers\n",
      " |              and function_invoked_handlers is to use the add_function_invoking_handler\n",
      " |              and add_function_invoked_handler methods.\n",
      " |\n",
      " |  async add_embedding_to_object(self, inputs: Union[~TDataModel, collections.abc.Sequence[~TDataModel]], field_to_embed: str, field_to_store: str, execution_settings: dict[str, 'PromptExecutionSettings'], container_mode: bool = False, cast_function: collections.abc.Callable[[list[float]], typing.Any] | None = None, **kwargs: Any)\n",
      " |      Gather all fields to embed, batch the embedding generation and store.\n",
      " |\n",
      " |  async invoke(self, function: 'KernelFunction | None' = None, arguments: semantic_kernel.functions.kernel_arguments.KernelArguments | None = None, function_name: str | None = None, plugin_name: str | None = None, metadata: dict[str, typing.Any] = {}, **kwargs: Any) -> semantic_kernel.functions.function_result.FunctionResult | None\n",
      " |      Execute a function and return the FunctionResult.\n",
      " |\n",
      " |      Args:\n",
      " |          function (KernelFunction): The function or functions to execute,\n",
      " |              this value has precedence when supplying both this and using function_name and plugin_name,\n",
      " |              if this is none, function_name and plugin_name are used and cannot be None.\n",
      " |          arguments (KernelArguments): The arguments to pass to the function(s), optional\n",
      " |          function_name (str | None): The name of the function to execute\n",
      " |          plugin_name (str | None): The name of the plugin to execute\n",
      " |          metadata (dict[str, Any]): The metadata to pass to the function(s)\n",
      " |          kwargs (dict[str, Any]): arguments that can be used instead of supplying KernelArguments\n",
      " |\n",
      " |      Raises:\n",
      " |          KernelInvokeException: If an error occurs during function invocation\n",
      " |\n",
      " |  async invoke_function_call(self, function_call: semantic_kernel.contents.function_call_content.FunctionCallContent, chat_history: semantic_kernel.contents.chat_history.ChatHistory, arguments: 'KernelArguments | None' = None, function_call_count: int | None = None, request_index: int | None = None, function_behavior: 'FunctionChoiceBehavior' = None) -> 'AutoFunctionInvocationContext | None'\n",
      " |      Processes the provided FunctionCallContent and updates the chat history.\n",
      " |\n",
      " |  async invoke_prompt(self, prompt: str, function_name: str | None = None, plugin_name: str | None = None, arguments: semantic_kernel.functions.kernel_arguments.KernelArguments | None = None, template_format: Literal['semantic-kernel', 'handlebars', 'jinja2'] = 'semantic-kernel', **kwargs: Any) -> semantic_kernel.functions.function_result.FunctionResult | None\n",
      " |      Invoke a function from the provided prompt.\n",
      " |\n",
      " |      Args:\n",
      " |          prompt (str): The prompt to use\n",
      " |          function_name (str): The name of the function, optional\n",
      " |          plugin_name (str): The name of the plugin, optional\n",
      " |          arguments (KernelArguments | None): The arguments to pass to the function(s), optional\n",
      " |          template_format (str | None): The format of the prompt template\n",
      " |          kwargs (dict[str, Any]): arguments that can be used instead of supplying KernelArguments\n",
      " |\n",
      " |      Returns:\n",
      " |          FunctionResult | list[FunctionResult] | None: The result of the function(s)\n",
      " |\n",
      " |  async invoke_prompt_stream(self, prompt: str, function_name: str | None = None, plugin_name: str | None = None, arguments: semantic_kernel.functions.kernel_arguments.KernelArguments | None = None, template_format: Literal['semantic-kernel', 'handlebars', 'jinja2'] = 'semantic-kernel', return_function_results: bool | None = False, **kwargs: Any) -> collections.abc.AsyncIterable[list['StreamingContentMixin'] | semantic_kernel.functions.function_result.FunctionResult | list[semantic_kernel.functions.function_result.FunctionResult]]\n",
      " |      Invoke a function from the provided prompt and stream the results.\n",
      " |\n",
      " |      Args:\n",
      " |          prompt (str): The prompt to use\n",
      " |          function_name (str): The name of the function, optional\n",
      " |          plugin_name (str): The name of the plugin, optional\n",
      " |          arguments (KernelArguments | None): The arguments to pass to the function(s), optional\n",
      " |          template_format (str | None): The format of the prompt template\n",
      " |          return_function_results (bool): If True, the function results are yielded as a list[FunctionResult]\n",
      " |          kwargs (dict[str, Any]): arguments that can be used instead of supplying KernelArguments\n",
      " |\n",
      " |      Returns:\n",
      " |          AsyncIterable[StreamingContentMixin]: The content of the stream of the last function provided.\n",
      " |\n",
      " |  async invoke_stream(self, function: 'KernelFunction | None' = None, arguments: semantic_kernel.functions.kernel_arguments.KernelArguments | None = None, function_name: str | None = None, plugin_name: str | None = None, metadata: dict[str, typing.Any] = {}, return_function_results: bool = False, **kwargs: Any) -> collections.abc.AsyncGenerator[list['StreamingContentMixin'] | semantic_kernel.functions.function_result.FunctionResult | list[semantic_kernel.functions.function_result.FunctionResult], typing.Any]\n",
      " |      Execute one or more stream functions.\n",
      " |\n",
      " |      This will execute the functions in the order they are provided, if a list of functions is provided.\n",
      " |      When multiple functions are provided only the last one is streamed, the rest is executed as a pipeline.\n",
      " |\n",
      " |      Args:\n",
      " |          function (KernelFunction): The function to execute,\n",
      " |              this value has precedence when supplying both this and using function_name and plugin_name,\n",
      " |              if this is none, function_name and plugin_name are used and cannot be None.\n",
      " |          arguments (KernelArguments | None): The arguments to pass to the function(s), optional\n",
      " |          function_name (str | None): The name of the function to execute\n",
      " |          plugin_name (str | None): The name of the plugin to execute\n",
      " |          metadata (dict[str, Any]): The metadata to pass to the function(s)\n",
      " |          return_function_results (bool): If True, the function results are yielded as a list[FunctionResult]\n",
      " |          in addition to the streaming content, otherwise only the streaming content is yielded.\n",
      " |          kwargs (dict[str, Any]): arguments that can be used instead of supplying KernelArguments\n",
      " |\n",
      " |      Yields:\n",
      " |          StreamingContentMixin: The content of the stream of the last function provided.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {}\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __private_attributes__ = {}\n",
      " |\n",
      " |  __pydantic_complete__ = True\n",
      " |\n",
      " |  __pydantic_core_schema__ = {'definitions': [{'cls': <class 'semantic_k...\n",
      " |\n",
      " |  __pydantic_custom_init__ = True\n",
      " |\n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |\n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |\n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |\n",
      " |  __pydantic_post_init__ = None\n",
      " |\n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |\n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"Kernel\", validator=Mod...\n",
      " |\n",
      " |  __signature__ = <Signature (plugins: semantic_kernel.functions.k...R_C...\n",
      " |\n",
      " |  model_computed_fields = {}\n",
      " |\n",
      " |  model_config = {'arbitrary_types_allowed': True, 'populate_by_name': T...\n",
      " |\n",
      " |  model_fields = {'ai_service_selector': FieldInfo(annotation=AIServiceS...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from semantic_kernel.filters.kernel_filters_extension.KernelFilterExtension:\n",
      " |\n",
      " |  add_filter(self, filter_type: Union[Literal[<FilterTypes.AUTO_FUNCTION_INVOCATION: 'auto_function_invocation'>, <FilterTypes.FUNCTION_INVOCATION: 'function_invocation'>, <FilterTypes.PROMPT_RENDERING: 'prompt_rendering'>], semantic_kernel.filters.filter_types.FilterTypes], filter: collections.abc.Callable[[~FILTER_CONTEXT_TYPE, collections.abc.Callable[[~FILTER_CONTEXT_TYPE], None]], None]) -> None\n",
      " |      Add a filter to the Kernel.\n",
      " |\n",
      " |      Each filter is added to the beginning of the list of filters,\n",
      " |      this is because the filters are executed in the order they are added,\n",
      " |      so the first filter added, will be the first to be executed,\n",
      " |      but it will also be the last executed for the part after `await next(context)`.\n",
      " |\n",
      " |      Args:\n",
      " |          filter_type (str): The type of the filter to add (function_invocation, prompt_rendering)\n",
      " |          filter (object): The filter to add\n",
      " |\n",
      " |      Raises:\n",
      " |          FilterDefinitionException: If an error occurs while adding the filter to the kernel\n",
      " |\n",
      " |  construct_call_stack(self, filter_type: semantic_kernel.filters.filter_types.FilterTypes, inner_function: collections.abc.Callable[[~FILTER_CONTEXT_TYPE], collections.abc.Coroutine[typing.Any, typing.Any, None]]) -> collections.abc.Callable[[~FILTER_CONTEXT_TYPE], collections.abc.Coroutine[typing.Any, typing.Any, None]]\n",
      " |      Construct the call stack for the given filter type.\n",
      " |\n",
      " |  filter(self, filter_type: Union[Literal[<FilterTypes.AUTO_FUNCTION_INVOCATION: 'auto_function_invocation'>, <FilterTypes.FUNCTION_INVOCATION: 'function_invocation'>, <FilterTypes.PROMPT_RENDERING: 'prompt_rendering'>], semantic_kernel.filters.filter_types.FilterTypes]) -> collections.abc.Callable[[collections.abc.Callable[[~FILTER_CONTEXT_TYPE, collections.abc.Callable[[~FILTER_CONTEXT_TYPE], None]], None]], collections.abc.Callable[[~FILTER_CONTEXT_TYPE, collections.abc.Callable[[~FILTER_CONTEXT_TYPE], None]], None]]\n",
      " |      Decorator to add a filter to the Kernel.\n",
      " |\n",
      " |  remove_filter(self, filter_type: Union[Literal[<FilterTypes.AUTO_FUNCTION_INVOCATION: 'auto_function_invocation'>, <FilterTypes.FUNCTION_INVOCATION: 'function_invocation'>, <FilterTypes.PROMPT_RENDERING: 'prompt_rendering'>], semantic_kernel.filters.filter_types.FilterTypes, NoneType] = None, filter_id: int | None = None, position: int | None = None) -> None\n",
      " |      Remove a filter from the Kernel.\n",
      " |\n",
      " |      Args:\n",
      " |          filter_type (str | FilterTypes | None):\n",
      " |              The type of the filter to remove.\n",
      " |          filter_id (int): The id of the hook to remove\n",
      " |          position (int): The position of the filter in the list\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from semantic_kernel.functions.kernel_function_extension.KernelFunctionExtension:\n",
      " |\n",
      " |  add_function(self, plugin_name: str, function: 'KERNEL_FUNCTION_TYPE | None' = None, function_name: str | None = None, description: str | None = None, prompt: str | None = None, prompt_template_config: semantic_kernel.prompt_template.prompt_template_config.PromptTemplateConfig | None = None, prompt_execution_settings: semantic_kernel.connectors.ai.prompt_execution_settings.PromptExecutionSettings | list[semantic_kernel.connectors.ai.prompt_execution_settings.PromptExecutionSettings] | dict[str, semantic_kernel.connectors.ai.prompt_execution_settings.PromptExecutionSettings] | None = None, template_format: Literal['semantic-kernel', 'handlebars', 'jinja2'] = 'semantic-kernel', prompt_template: semantic_kernel.prompt_template.prompt_template_base.PromptTemplateBase | None = None, return_plugin: bool = False, **kwargs: Any) -> 'KernelFunction | KernelPlugin'\n",
      " |      Adds a function to the specified plugin.\n",
      " |\n",
      " |      Args:\n",
      " |          plugin_name (str): The name of the plugin to add the function to\n",
      " |          function (KernelFunction | Callable[..., Any]): The function to add\n",
      " |          function_name (str): The name of the function\n",
      " |          plugin_name (str): The name of the plugin\n",
      " |          description (str | None): The description of the function\n",
      " |          prompt (str | None): The prompt template.\n",
      " |          prompt_template_config (PromptTemplateConfig | None): The prompt template configuration\n",
      " |          prompt_execution_settings: The execution settings, will be parsed into a dict.\n",
      " |          template_format (str | None): The format of the prompt template\n",
      " |          prompt_template (PromptTemplateBase | None): The prompt template\n",
      " |          return_plugin (bool): If True, the plugin is returned instead of the function\n",
      " |          kwargs (Any): Additional arguments\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelFunction | KernelPlugin: The function that was added, or the plugin if return_plugin is True\n",
      " |\n",
      " |  add_functions(self, plugin_name: str, functions: 'list[KERNEL_FUNCTION_TYPE] | dict[str, KERNEL_FUNCTION_TYPE]') -> 'KernelPlugin'\n",
      " |      Adds a list of functions to the specified plugin.\n",
      " |\n",
      " |      Args:\n",
      " |          plugin_name (str): The name of the plugin to add the functions to\n",
      " |          functions (list[KernelFunction] | dict[str, KernelFunction]): The functions to add\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelPlugin: The plugin that the functions were added to.\n",
      " |\n",
      " |  add_plugin(self, plugin: semantic_kernel.functions.kernel_plugin.KernelPlugin | object | dict[str, typing.Any] | None = None, plugin_name: str | None = None, parent_directory: str | None = None, description: str | None = None, class_init_arguments: dict[str, dict[str, typing.Any]] | None = None) -> 'KernelPlugin'\n",
      " |      Adds a plugin to the kernel's collection of plugins.\n",
      " |\n",
      " |      If a plugin is provided, it uses that instance instead of creating a new KernelPlugin.\n",
      " |      See KernelPlugin.from_directory for more details on how the directory is parsed.\n",
      " |\n",
      " |      Args:\n",
      " |          plugin: The plugin to add.\n",
      " |              This can be a KernelPlugin, in which case it is added straightaway and other parameters are ignored,\n",
      " |              a custom class that contains methods with the kernel_function decorator\n",
      " |              or a dictionary of functions with the kernel_function decorator for one or\n",
      " |              several methods.\n",
      " |          plugin_name: The name of the plugin, used if the plugin is not a KernelPlugin,\n",
      " |              if the plugin is None and the parent_directory is set,\n",
      " |              KernelPlugin.from_directory is called with those parameters,\n",
      " |              see `KernelPlugin.from_directory` for details.\n",
      " |          parent_directory: The parent directory path where the plugin directory resides\n",
      " |          description: The description of the plugin, used if the plugin is not a KernelPlugin.\n",
      " |          class_init_arguments: The class initialization arguments\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelPlugin: The plugin that was added.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If a KernelPlugin needs to be created, but it is not valid.\n",
      " |\n",
      " |  async add_plugin_from_openai(self, plugin_name: str, plugin_url: str | None = None, plugin_str: str | None = None, execution_parameters: 'OpenAIFunctionExecutionParameters | None' = None, description: str | None = None) -> semantic_kernel.functions.kernel_plugin.KernelPlugin\n",
      " |      Add a plugin from an OpenAPI document.\n",
      " |\n",
      " |      Args:\n",
      " |          plugin_name (str): The name of the plugin\n",
      " |          plugin_url (str | None): The URL of the plugin\n",
      " |          plugin_str (str | None): The JSON string of the plugin\n",
      " |          execution_parameters (OpenAIFunctionExecutionParameters | None): The execution parameters\n",
      " |          description (str | None): The description of the plugin\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelPlugin: The imported plugin\n",
      " |\n",
      " |      Raises:\n",
      " |          PluginInitializationError: if the plugin URL or plugin JSON/YAML is not provided\n",
      " |\n",
      " |  add_plugin_from_openapi(self, plugin_name: str, openapi_document_path: str | None = None, openapi_parsed_spec: dict[str, typing.Any] | None = None, execution_settings: 'OpenAPIFunctionExecutionParameters | None' = None, description: str | None = None) -> semantic_kernel.functions.kernel_plugin.KernelPlugin\n",
      " |      Add a plugin from the OpenAPI manifest.\n",
      " |\n",
      " |      Args:\n",
      " |          plugin_name: The name of the plugin\n",
      " |          openapi_document_path: The path to the OpenAPI document\n",
      " |          openapi_parsed_spec: The parsed OpenAPI spec\n",
      " |          execution_settings: The execution parameters\n",
      " |          description: The description of the plugin\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelPlugin: The imported plugin\n",
      " |\n",
      " |      Raises:\n",
      " |          PluginInitializationError: if the plugin URL or plugin JSON/YAML is not provided\n",
      " |\n",
      " |  add_plugins(self, plugins: list[semantic_kernel.functions.kernel_plugin.KernelPlugin] | dict[str, semantic_kernel.functions.kernel_plugin.KernelPlugin | object]) -> None\n",
      " |      Adds a list of plugins to the kernel's collection of plugins.\n",
      " |\n",
      " |      Args:\n",
      " |          plugins (list[KernelPlugin] | dict[str, KernelPlugin]): The plugins to add to the kernel\n",
      " |\n",
      " |  get_full_list_of_function_metadata(self) -> list['KernelFunctionMetadata']\n",
      " |      Get a list of all function metadata in the plugins.\n",
      " |\n",
      " |  get_function(self, plugin_name: str | None, function_name: str) -> 'KernelFunction'\n",
      " |      Get a function by plugin_name and function_name.\n",
      " |\n",
      " |      Args:\n",
      " |          plugin_name (str | None): The name of the plugin\n",
      " |          function_name (str): The name of the function\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelFunction: The function\n",
      " |\n",
      " |      Raises:\n",
      " |          KernelPluginNotFoundError: If the plugin is not found\n",
      " |          KernelFunctionNotFoundError: If the function is not found\n",
      " |\n",
      " |  get_function_from_fully_qualified_function_name(self, fully_qualified_function_name: str) -> 'KernelFunction'\n",
      " |      Get a function by its fully qualified name (<plugin_name>-<function_name>).\n",
      " |\n",
      " |      Args:\n",
      " |          fully_qualified_function_name (str): The fully qualified name of the function,\n",
      " |              if there is no '-' in the name, it is assumed that it is only a function_name.\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelFunction: The function\n",
      " |\n",
      " |      Raises:\n",
      " |          KernelPluginNotFoundError: If the plugin is not found\n",
      " |          KernelFunctionNotFoundError: If the function is not found\n",
      " |\n",
      " |  get_list_of_function_metadata(self, *args: Any, **kwargs: Any) -> list['KernelFunctionMetadata']\n",
      " |      Get a list of all function metadata in the plugin collection.\n",
      " |\n",
      " |  get_list_of_function_metadata_bool(self, include_prompt: bool = True, include_native: bool = True) -> list['KernelFunctionMetadata']\n",
      " |      Get a list of the function metadata in the plugin collection.\n",
      " |\n",
      " |      Args:\n",
      " |          include_prompt (bool): Whether to include semantic functions in the list.\n",
      " |          include_native (bool): Whether to include native functions in the list.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of KernelFunctionMetadata objects in the collection.\n",
      " |\n",
      " |  get_list_of_function_metadata_filters(self, filters: dict[typing.Literal['excluded_plugins', 'included_plugins', 'excluded_functions', 'included_functions'], list[str]]) -> list['KernelFunctionMetadata']\n",
      " |      Get a list of Kernel Function Metadata based on filters.\n",
      " |\n",
      " |      Args:\n",
      " |          filters (dict[str, list[str]]): The filters to apply to the function list.\n",
      " |              The keys are:\n",
      " |                  - included_plugins: A list of plugin names to include.\n",
      " |                  - excluded_plugins: A list of plugin names to exclude.\n",
      " |                  - included_functions: A list of function names to include.\n",
      " |                  - excluded_functions: A list of function names to exclude.\n",
      " |              The included and excluded parameters are mutually exclusive.\n",
      " |              The function names are checked against the fully qualified name of a function.\n",
      " |\n",
      " |      Returns:\n",
      " |          list[KernelFunctionMetadata]: The list of Kernel Function Metadata that match the filters.\n",
      " |\n",
      " |  get_plugin(self, plugin_name: str) -> 'KernelPlugin'\n",
      " |      Get a plugin by name.\n",
      " |\n",
      " |      Args:\n",
      " |          plugin_name (str): The name of the plugin\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelPlugin: The plugin\n",
      " |\n",
      " |      Raises:\n",
      " |          KernelPluginNotFoundError: If the plugin is not found\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from semantic_kernel.functions.kernel_function_extension.KernelFunctionExtension:\n",
      " |\n",
      " |  rewrite_plugins(plugins: semantic_kernel.functions.kernel_plugin.KernelPlugin | list[semantic_kernel.functions.kernel_plugin.KernelPlugin] | dict[str, semantic_kernel.functions.kernel_plugin.KernelPlugin] | None = None) -> dict[str, semantic_kernel.functions.kernel_plugin.KernelPlugin]\n",
      " |      Rewrite plugins to a dictionary.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from semantic_kernel.services.kernel_services_extension.KernelServicesExtension:\n",
      " |\n",
      " |  add_service(self, service: semantic_kernel.services.ai_service_client_base.AIServiceClientBase, overwrite: bool = False) -> None\n",
      " |      Add a single service to the Kernel.\n",
      " |\n",
      " |      Args:\n",
      " |          service (AIServiceClientBase): The service to add.\n",
      " |          overwrite (bool, optional): Whether to overwrite the service if it already exists. Defaults to False.\n",
      " |\n",
      " |  get_prompt_execution_settings_from_service_id(self, service_id: str, type: type[~AI_SERVICE_CLIENT_TYPE] | None = None) -> semantic_kernel.connectors.ai.prompt_execution_settings.PromptExecutionSettings\n",
      " |      Get the specific request settings from the service, instantiated with the service_id and ai_model_id.\n",
      " |\n",
      " |  get_service(self, service_id: str | None = None, type: type[~AI_SERVICE_CLIENT_TYPE] | tuple[type[~AI_SERVICE_CLIENT_TYPE], ...] | None = None) -> semantic_kernel.services.ai_service_client_base.AIServiceClientBase\n",
      " |      Get a service by service_id and type.\n",
      " |\n",
      " |      Type is optional and when not supplied, no checks are done.\n",
      " |      Type should be\n",
      " |          TextCompletionClientBase, ChatCompletionClientBase, EmbeddingGeneratorBase\n",
      " |          or a subclass of one.\n",
      " |          You can also check for multiple types in one go,\n",
      " |          by using a tuple: (TextCompletionClientBase, ChatCompletionClientBase).\n",
      " |\n",
      " |      If type and service_id are both None, the first service is returned.\n",
      " |\n",
      " |      Args:\n",
      " |          service_id (str | None): The service id,\n",
      " |              if None, the default service is returned or the first service is returned.\n",
      " |          type (Type[AI_SERVICE_CLIENT_TYPE] | tuple[type[AI_SERVICE_CLIENT_TYPE], ...] | None):\n",
      " |              The type of the service, if None, no checks are done on service type.\n",
      " |\n",
      " |      Returns:\n",
      " |          AIServiceClientBase: The service, should be a class derived from AIServiceClientBase.\n",
      " |\n",
      " |      Raises:\n",
      " |          KernelServiceNotFoundError: If no service is found that matches the type or id.\n",
      " |\n",
      " |  get_services_by_type(self, type: type[~AI_SERVICE_CLIENT_TYPE] | tuple[type[~AI_SERVICE_CLIENT_TYPE], ...] | None) -> dict[str, semantic_kernel.services.ai_service_client_base.AIServiceClientBase]\n",
      " |      Get all services of a specific type.\n",
      " |\n",
      " |  remove_all_services(self) -> None\n",
      " |      Removes the services from the Kernel, does not delete them.\n",
      " |\n",
      " |  remove_service(self, service_id: str) -> None\n",
      " |      Delete a single service from the Kernel.\n",
      " |\n",
      " |  select_ai_service(self, function: 'KernelFunction', arguments: 'KernelArguments') -> tuple[semantic_kernel.services.ai_service_client_base.AIServiceClientBase, semantic_kernel.connectors.ai.prompt_execution_settings.PromptExecutionSettings]\n",
      " |      Uses the AI service selector to select a service for the function.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from semantic_kernel.services.kernel_services_extension.KernelServicesExtension:\n",
      " |\n",
      " |  rewrite_services(services: Union[~AI_SERVICE_CLIENT_TYPE, list[~AI_SERVICE_CLIENT_TYPE], dict[str, ~AI_SERVICE_CLIENT_TYPE], NoneType] = None) -> dict[str, ~AI_SERVICE_CLIENT_TYPE]\n",
      " |      Rewrite services to a dictionary.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from semantic_kernel.kernel_pydantic.KernelBaseModel:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |\n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |\n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |\n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |\n",
      " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |\n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      " |\n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |\n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |\n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |\n",
      " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |\n",
      " |      If you need `include` or `exclude`, use:\n",
      " |\n",
      " |      ```py\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |\n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |\n",
      " |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      " |\n",
      " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      " |\n",
      " |  model_copy(self, *, update: 'dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/serialization/#model_copy\n",
      " |\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |\n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |\n",
      " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/serialization/#modelmodel_dump\n",
      " |\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |\n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/serialization/#modelmodel_dump_json\n",
      " |\n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |\n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |\n",
      " |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |\n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema'\n",
      " |      Hook into generating the model's CoreSchema.\n",
      " |\n",
      " |      Args:\n",
      " |          source: The class we are generating a schema for.\n",
      " |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      " |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      " |\n",
      " |      Returns:\n",
      " |          A `pydantic-core` `CoreSchema`.\n",
      " |\n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |\n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |\n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called.\n",
      " |\n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |\n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by pydantic.\n",
      " |\n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |\n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |\n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |\n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |\n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |\n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |\n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |\n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |\n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |\n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |\n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |\n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'dict[str, Any] | None' = None) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |\n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |\n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |\n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |\n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |\n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/json/#json-parsing\n",
      " |\n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |\n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |\n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |\n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |\n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str'\n",
      " |\n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |\n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |\n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |\n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __pydantic_extra__\n",
      " |\n",
      " |  __pydantic_fields_set__\n",
      " |\n",
      " |  __pydantic_private__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __pydantic_root_model__ = False\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "print(help(request=kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__repr__', '__repr_args__', '__repr_name__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_inner_auto_function_invoke_handler', '_iter', 'add_embedding_to_object', 'add_filter', 'add_function', 'add_functions', 'add_plugin', 'add_plugin_from_openai', 'add_plugin_from_openapi', 'add_plugins', 'add_service', 'ai_service_selector', 'auto_function_invocation_filters', 'construct', 'construct_call_stack', 'copy', 'dict', 'filter', 'from_orm', 'function_invocation_filters', 'get_full_list_of_function_metadata', 'get_function', 'get_function_from_fully_qualified_function_name', 'get_list_of_function_metadata', 'get_list_of_function_metadata_bool', 'get_list_of_function_metadata_filters', 'get_plugin', 'get_prompt_execution_settings_from_service_id', 'get_service', 'get_services_by_type', 'invoke', 'invoke_function_call', 'invoke_prompt', 'invoke_prompt_stream', 'invoke_stream', 'json', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'plugins', 'prompt_rendering_filters', 'remove_all_services', 'remove_filter', 'remove_service', 'retry_mechanism', 'rewrite_plugins', 'rewrite_services', 'schema', 'schema_json', 'select_ai_service', 'services', 'update_forward_refs', 'validate']\n"
     ]
    }
   ],
   "source": [
    "print(dir(kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "gpt35 = OpenAIChatCompletion(ai_model_id=\"gpt-3.5-turbo\")\n",
    "gpt4o = OpenAIChatCompletion(ai_model_id=\"gpt-4o\")\n",
    "\n",
    "kernel.add_service(service=gpt35)\n",
    "kernel.add_service(service=gpt4o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To send the prompt to the service, we need to use a method called **`create_semantic_function`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a simple prompt\n",
    "\n",
    "1. **Load the prompt in a string variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Finish the following knock-knock joke. Knock, knock. Who's there? Dishes. Dishes who?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a *`function`* by using the **`add_function`** method of the kernel. The *`function_name`* and *`plugin_name`* parameters of the function are required but are not used, so you can give your function and plugin whatever name you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_function = kernel.add_function(\n",
    "    function_name=\"subrata\",\n",
    "    plugin_name=\"sample\",\n",
    "    prompt=prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Call the function. Note that all invocation methods are *`Asynchronous`*, so we need to use *`await`* to wait for their return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function failed. Error: Error occurred while invoking function subrata: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.OpenAIChatCompletion'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\"))\n",
      "Something went wrong in function invocation. During function invocation: 'sample-subrata'. Error description: 'Error occurred while invoking function subrata: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.OpenAIChatCompletion'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\"))'\n"
     ]
    },
    {
     "ename": "KernelInvokeException",
     "evalue": "Error occurred while invoking function: 'sample-subrata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:87\u001b[0m, in \u001b[0;36mOpenAIHandler._send_completion_request\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m     86\u001b[0m         settings_dict\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 87\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings_dict)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:1661\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1660\u001b[0m validate_response_format(response_format)\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1663\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1664\u001b[0m         {\n\u001b[1;32m   1665\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1666\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1667\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1668\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1669\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1670\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1671\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1672\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1673\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1674\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1675\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1676\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1677\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1678\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1679\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   1680\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1681\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1682\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1683\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1684\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1685\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1686\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1687\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1688\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1689\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1690\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1691\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1692\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1693\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1694\u001b[0m         },\n\u001b[1;32m   1695\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1696\u001b[0m     ),\n\u001b[1;32m   1697\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1698\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1699\u001b[0m     ),\n\u001b[1;32m   1700\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1701\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1702\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1703\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1843\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1840\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1841\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1842\u001b[0m )\n\u001b[0;32m-> 1843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1537\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1537\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1538\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1539\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1540\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1541\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1542\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1543\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1623\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1624\u001b[0m         input_options,\n\u001b[1;32m   1625\u001b[0m         cast_to,\n\u001b[1;32m   1626\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1627\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1628\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1629\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1630\u001b[0m     )\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1670\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1671\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1672\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1673\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1674\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1675\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1676\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1623\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1624\u001b[0m         input_options,\n\u001b[1;32m   1625\u001b[0m         cast_to,\n\u001b[1;32m   1626\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1627\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1628\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1629\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1630\u001b[0m     )\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1670\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1671\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1672\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1673\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1674\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1675\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1676\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1638\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1637\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1641\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1642\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1646\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1647\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mServiceResponseException\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/semantic_kernel/functions/kernel_function_from_prompt.py:173\u001b[0m, in \u001b[0;36mKernelFunctionFromPrompt._invoke_internal\u001b[0;34m(self, context)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     chat_message_contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m prompt_render_result\u001b[38;5;241m.\u001b[39mai_service\u001b[38;5;241m.\u001b[39mget_chat_message_contents(\n\u001b[1;32m    174\u001b[0m         chat_history\u001b[38;5;241m=\u001b[39mchat_history,\n\u001b[1;32m    175\u001b[0m         settings\u001b[38;5;241m=\u001b[39mprompt_render_result\u001b[38;5;241m.\u001b[39mexecution_settings,\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m\"\u001b[39m: context\u001b[38;5;241m.\u001b[39mkernel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments\u001b[39m\u001b[38;5;124m\"\u001b[39m: context\u001b[38;5;241m.\u001b[39marguments},\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/semantic_kernel/connectors/ai/chat_completion_client_base.py:134\u001b[0m, in \u001b[0;36mChatCompletionClientBase.get_chat_message_contents\u001b[0;34m(self, chat_history, settings, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m     settings\u001b[38;5;241m.\u001b[39mfunction_choice_behavior \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mfunction_choice_behavior\u001b[38;5;241m.\u001b[39mauto_invoke_kernel_functions\n\u001b[1;32m    133\u001b[0m ):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_get_chat_message_contents(chat_history, settings)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Auto invoke loop\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/semantic_kernel/utils/telemetry/model_diagnostics/decorators.py:83\u001b[0m, in \u001b[0;36mtrace_chat_completion.<locals>.inner_trace_chat_completion.<locals>.wrapper_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m completion_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     85\u001b[0m completion_service: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatCompletionClientBase\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_chat_completion_base.py:88\u001b[0m, in \u001b[0;36mOpenAIChatCompletionBase._inner_get_chat_message_contents\u001b[0;34m(self, chat_history, settings)\u001b[0m\n\u001b[1;32m     86\u001b[0m settings\u001b[38;5;241m.\u001b[39mai_model_id \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mai_model_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_id\n\u001b[0;32m---> 88\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(settings)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ChatCompletion)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:59\u001b[0m, in \u001b[0;36mOpenAIHandler._send_request\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_completion_request(settings)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_type \u001b[38;5;241m==\u001b[39m OpenAIModelTypes\u001b[38;5;241m.\u001b[39mEMBEDDING:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:104\u001b[0m, in \u001b[0;36mOpenAIHandler._send_completion_request\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service failed to complete the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    106\u001b[0m         ex,\n\u001b[1;32m    107\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n",
      "\u001b[0;31mServiceResponseException\u001b[0m: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.OpenAIChatCompletion'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\"))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFunctionExecutionException\u001b[0m                Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/semantic_kernel/kernel.py:201\u001b[0m, in \u001b[0;36mKernel.invoke\u001b[0;34m(self, function, arguments, function_name, plugin_name, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m function\u001b[38;5;241m.\u001b[39minvoke(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, arguments\u001b[38;5;241m=\u001b[39marguments, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationCancelledException \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/semantic_kernel/functions/kernel_function.py:256\u001b[0m, in \u001b[0;36mKernelFunction.invoke\u001b[0;34m(self, kernel, arguments, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(current_span, e, attributes)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/semantic_kernel/functions/kernel_function.py:248\u001b[0m, in \u001b[0;36mKernelFunction.invoke\u001b[0;34m(self, kernel, arguments, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m stack \u001b[38;5;241m=\u001b[39m kernel\u001b[38;5;241m.\u001b[39mconstruct_call_stack(\n\u001b[1;32m    245\u001b[0m     filter_type\u001b[38;5;241m=\u001b[39mFilterTypes\u001b[38;5;241m.\u001b[39mFUNCTION_INVOCATION,\n\u001b[1;32m    246\u001b[0m     inner_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_internal,\n\u001b[1;32m    247\u001b[0m )\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m stack(function_context)\n\u001b[1;32m    250\u001b[0m KernelFunctionLogMessages\u001b[38;5;241m.\u001b[39mlog_function_invoked_success(logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfully_qualified_name)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/semantic_kernel/functions/kernel_function_from_prompt.py:179\u001b[0m, in \u001b[0;36mKernelFunctionFromPrompt._invoke_internal\u001b[0;34m(self, context)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FunctionExecutionException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred while invoking function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chat_message_contents:\n",
      "\u001b[0;31mFunctionExecutionException\u001b[0m: Error occurred while invoking function subrata: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.OpenAIChatCompletion'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\"))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKernelInvokeException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m kernel\u001b[38;5;241m.\u001b[39minvoke(function\u001b[38;5;241m=\u001b[39mprompt_function, request\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/agents-tutorial-lffBnLp0-py3.12/lib/python3.12/site-packages/semantic_kernel/kernel.py:210\u001b[0m, in \u001b[0;36mKernel.invoke\u001b[0;34m(self, function, arguments, function_name, plugin_name, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    206\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSomething went wrong in function invocation. During function invocation:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39mfully_qualified_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Error description: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m     )\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m KernelInvokeException(\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred while invoking function: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39mfully_qualified_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mKernelInvokeException\u001b[0m: Error occurred while invoking function: 'sample-subrata'"
     ]
    }
   ],
   "source": [
    "response = await kernel.invoke(function=prompt_function, request=prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dishes a really bad joke!\n"
     ]
    }
   ],
   "source": [
    "# Entire Code in One Place\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    OpenAIChatCompletion,\n",
    "    AzureChatCompletion,\n",
    ")\n",
    "\n",
    "# Initialize the Kernel\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "# Define Connectors\n",
    "gpt35 = AzureChatCompletion(deployment_name=\"2024-08-01-preview\")\n",
    "gpt4o = OpenAIChatCompletion(ai_model_id=\"gpt-4o\")\n",
    "\n",
    "# Add the connectors to the kernel's service\n",
    "kernel.add_service(service=gpt35)\n",
    "kernel.add_service(service=gpt4o)\n",
    "\n",
    "# Write a prompt\n",
    "prompt = \"Finish the following knock-knock joke. Knock, knock. Who's there? Dishes. Dishes who?\"\n",
    "\n",
    "# Create a function (semantic) using add_function method of the kernel\n",
    "prompt_function = kernel.add_function(\n",
    "    function_name=\"Subrata\",\n",
    "    plugin_name=\"Mondal\",\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "# Invoke the created function\n",
    "response = await kernel.invoke(\n",
    "    function=prompt_function,\n",
    "    request=prompt,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Functions and Native Functions\n",
    "\n",
    "- **Semantic Functions:** are functions that connect to AI Services (LLM) to perform a task. The default parameter for all semantic functions is called *`input`*.\n",
    "\n",
    "- **Native Functions:** are regular functions written in your programming language (Python).\n",
    "\n",
    "\n",
    "The reason to differentiate a Native Function from any other Regular Function in your code is that the native funcion will have additional attributes that will tell the Kernel what it does.\n",
    "\n",
    "After loading a native function into the Kernel, you can use it in chains that combine native and semantic functions. On top of it, **`Semantic Kernel Planner`** can use the function when creating a plan to achieve a user goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Semantic Function in Python\n",
    "\n",
    "1. **Adding parameter to the Semantic Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dishes the police, open up!\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "\n",
    "args = KernelArguments(input=\"Boo\")\n",
    "\n",
    "response = await kernel.invoke(\n",
    "    function=prompt_function,\n",
    "    request=prompt,\n",
    "    arguments=args,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Native Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Native Functions are created in the same language our application is using like Python. \n",
    "\n",
    "In Python, the **Native functions need to be inside a `Class`**. The Class used to be called **`Skill`** and now it's called **`Plugin`**.\n",
    "\n",
    "- **`Plugin:`** is just a *`collection of functions`*. You **cannot mix both the `Native` and `Semantic Function` in the same Plugin/Skill**. For, example we'll create a **`ShowManager` Plugin**.\n",
    "\n",
    "**How to create a Native Function?**\n",
    "\n",
    "To create a Native Function, we will use **`@kernel_function` decorator**. And the decorator must contain fields for **`description`** and **`name`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
    "\n",
    "\n",
    "# Plugin\n",
    "class ShowManager:\n",
    "    @kernel_function(\n",
    "        description=\"Randomly choose among a theme for a joke.\",\n",
    "        name=\"random_theme\",\n",
    "    )\n",
    "    def random_theme(self) -> str:\n",
    "        themes: list[str] = [\"Boo\", \"Dishes\", \"Art\", \"Needle\", \"Tank\", \"Police\"]\n",
    "        theme: str = random.choice(seq=themes)\n",
    "        return theme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to load the *Plugin (ShowManager)* and all its *functions* in the Kernel, we use the **`add_plugin`** method of the Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='ShowManager' description=None functions={'random_theme': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='random_theme', plugin_name='ShowManager', description='Randomly choose among a theme for a joke.', parameters=[], is_prompt=False, is_asynchronous=False, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x10e315490>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x10dbab140>, method=<bound method ShowManager.random_theme of <__main__.ShowManager object at 0x10e32fc50>>, stream_method=None)}\n"
     ]
    }
   ],
   "source": [
    "theme_choice_kernel_plugin = kernel.add_plugin(\n",
    "    plugin=ShowManager(), plugin_name=\"ShowManager\"\n",
    ")\n",
    "\n",
    "print(theme_choice_kernel_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call the Native Function from a *Plugin (ShowManager)*, simply put the name of the method within brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tank\n"
     ]
    }
   ],
   "source": [
    "response = await kernel.invoke(\n",
    "    function=theme_choice_kernel_plugin[\"random_theme\"],\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the greatest strengths of Microsoft Semantic Kernel is that you can create Semantic Plugins that are **Language Agnostic**. \n",
    "\n",
    "- **`Semantic Plugins:`** are **collections of Semantic Functions** that can be imported into the Kernel. It allows to separate your code from the AI Function which makes the code easier to maintain.\n",
    "\n",
    "> Each Semantic Function of Semantic Plugin is defined by a directory containing two text files:\n",
    "- **`config.json:`** contains the *`configuration`* for the semantic function like LLM, temperature, description and inputs.\n",
    "- **`skprompt.txt:`** contains the *`prompt`* of the semantic function which will be sent to the LLM to generate response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define a Plugin that contains two semantic functions. \n",
    "1. knock-knock joke generator function\n",
    "2. function that take jokes as an input and explains why it's funny.\n",
    "\n",
    "**Folder structure of Semantic Plugins:**\n",
    "\n",
    "<img src=\"../assets/plugins-folder-structure.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knock-knock joke semantic function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `config.json` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"schema\": 1,\n",
    "    \"type\": \"completion\",\n",
    "    \"description\": \"Generates a knock-knock joke based on user input\",\n",
    "    \"default_services\": [\n",
    "        \"gpt35\",\n",
    "        \"gpt4o\",\n",
    "    ],\n",
    "    \"execution_settings\": {\n",
    "        \"default\": {\n",
    "            \"temperature\": 0.8,\n",
    "            \"number_of_responses\": 1,\n",
    "            \"top_p\": 0.9,\n",
    "            \"max_tokens\": 4000,\n",
    "            \"presence_penalty\": 0.0,\n",
    "            \"frequency_penalty\": 0.0,\n",
    "        },\n",
    "    },\n",
    "    \"input_variables\": [\n",
    "        {\n",
    "            \"name\": \"input\",\n",
    "            \"description\": \"The topic that the Joke should be written about.\",\n",
    "            \"required\": true,\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "- **`default_services:`** property is an arrat of preferred engines (LLM) to use (in order). Since knock-knock jokes are simple, we're going to use GPT-3.5 for it. All the parameters in the json file are required.\n",
    "\n",
    "- **`description:`** field is important because it can be used by the **`Planner`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `skprompt.txt` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```plaintext\n",
    "You are given a joke with the following setup:\n",
    "Knock, knock!\n",
    "Who's there?\n",
    "{{$input}}!\n",
    "{{$input}} who?\n",
    "Repeat the whole setup and finish the joke with a funny punchline.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Joke Semantic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `config.json` file\n",
    "\n",
    "This file is almost exactly same as the config.json file used for the knock-knock joke function. We have made only 3 changes:\n",
    "\n",
    "- The description\n",
    "- The description of the *`input_variables`*\n",
    "- The *`default_serices`* field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"schema\": 1,\n",
    "    \"type\": \"completion\",\n",
    "    \"description\": \"Give a joke, explain why it is funny.\",\n",
    "    \"default_services\": [\n",
    "        \"gpt4o\",\n",
    "    ],\n",
    "    \"execution_settings\": {\n",
    "        \"default\": {\n",
    "            \"temperature\": 0.8,\n",
    "            \"number_of_responses\": 1,\n",
    "            \"top_p\": 0.9,\n",
    "            \"max_tokens\": 4000,\n",
    "            \"presence_penalty\": 0.0,\n",
    "            \"frequency_penalty\": 0.0,\n",
    "        },\n",
    "    },\n",
    "    \"input_variables\": [\n",
    "        {\n",
    "            \"name\": \"input\",\n",
    "            \"description\": \"The joke we want to explain.\",\n",
    "            \"required\": true,\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `skprompt.txt` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```plaintext\n",
    "You are given the following joke:\n",
    "{{$input}}\n",
    "First, tell the joke.\n",
    "Then, explain the joke\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Plugin using Python\n",
    "\n",
    "You can load all the functions inside a plugin directory using the **`add_plugin`** method from the Kernel object. Just set the first parameter to *`None`* and set the *`parent_direcory`* parameter to the directory where the plugin is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_plugin = kernel.add_plugin(\n",
    "    plugin=None,\n",
    "    parent_directory=\"./plugins\",\n",
    "    plugin_name=\"jokes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call the Functions in the same way as you would call a function from a Native Plugin by putting the function name within brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knock, knock!  \n",
      "Who's there?  \n",
      "Scientist!  \n",
      "Scientist who?  \n",
      "\n",
      "Scientist you a joke, but I wasn't sure if you'd react or remain inert!\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "\n",
    "theme = \"Scientist\"\n",
    "knock_knock_joke = await kernel.invoke(\n",
    "    function=jokes_plugin[\"knock_knock_joke\"],\n",
    "    arguments=KernelArguments(input=theme),\n",
    ")\n",
    "\n",
    "print(knock_knock_joke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the result of Knock-Knock Joke to the *`explain_joke`* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here it is:\n",
      "\n",
      "**Joke:**\n",
      "Why can't you trust an atom?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "**Explanation:**\n",
      "This joke plays on the double meaning of the phrase \"make up.\" In science, atoms are the basic building blocks of matter, so they \"make up\" everything in the physical world. However, in everyday language, saying someone \"makes up everything\" means they are not truthful or they fabricate stories. The humor comes from this wordplay, as it initially suggests that atoms are untrustworthy, but the punchline reveals it’s because they literally compose all matter.\n"
     ]
    }
   ],
   "source": [
    "joke_explanation = await kernel.invoke(\n",
    "    function=jokes_plugin[\"explain_joke\"],\n",
    "    arguments=KernelArguments(input=theme),\n",
    ")\n",
    "\n",
    "print(joke_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planner: Use it to run a Multistep Task.\n",
    "\n",
    "Instead of calling functions yourself, let the Microsoft Semantic Kernel choose the functions for you. This makes the code simpler and give the users ability to combine your code in ways that you haven't considered.\n",
    "\n",
    "- **Semantic Kernel has Two types of Plannner:**\n",
    "    - *`Function Calling Stepwise Planner: (Deprecated)`* This planner employs a step-by-step approach, utilizing OpenAI's function calling capabilities to iteratively select and execute functions. It is particularly suitable for complex tasks requiring sequential function execution. However, with advancements in AI models, the Function Calling Stepwise Planner is being **`deprecated`** in favor of automatic function calling, which offers more streamlined and efficient task execution.\n",
    "\n",
    "    - *`Handlebars planner: (Deprecated)`* This planner uses Handlebars syntax to generate plans, allowing the model to leverage native features like loops and conditions without additional prompting. It enables the creation of customized plans by defining templates that the AI can follow. However, the Handlebars Planner is also being *`deprecated`* in favor of more advanced planning methods that provide greater reliability and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-tutorial-lffBnLp0-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
